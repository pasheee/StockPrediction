{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5420c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from graphsParsing.graphs_parsing_tools import BinanceDataCollector\n",
    "import os\n",
    "\n",
    "\n",
    "# Define parameters for graph data collection\n",
    "symbol = 'BTCUSDT'\n",
    "interval = '5m'  \n",
    "days_back = 730  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f815fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "news = pd.read_csv('result_news_data.csv')\n",
    "\n",
    "# Convert 'date_time' to datetime format and filter out rows with invalid date formats\n",
    "news['date_length'] = news['date_time'].apply(lambda x: len(x)) \n",
    "filtered_news = news[news['date_length'] == 19] # filtering for date_time strings of length 19 (to convert to datetime, because 3 of them are not the same format)\n",
    "filtered_news = filtered_news.drop(columns=['date_length'])\n",
    "filtered_news['date_time'] = pd.to_datetime(filtered_news['date_time'])\n",
    "\n",
    "filtered_news.to_csv('newsAndPriceMapping/news.csv', index=False)\n",
    "\n",
    "# Sort the DataFrame by 'date_time'\n",
    "filtered_news = filtered_news.sort_values('date_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628f017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем сбор данных...\n",
      "Данные успешно собраны и сохранены в файл BTCUSDT_5m_365days.csv\n"
     ]
    }
   ],
   "source": [
    "# Collect historical data using BinanceDataCollector <------- UNCOMMENT THIS PART TO RUN DATA COLLECTION \n",
    "# collector = BinanceDataCollector()\n",
    "\n",
    "# symbol = 'BTCUSDT'  \n",
    "# interval = '5m'     \n",
    "# days_back = 365 \n",
    "\n",
    "# print(\"Начинаем сбор данных...\")\n",
    "# raw_data = collector.collect_historical_data(symbol, interval, days_back)\n",
    "\n",
    "# if raw_data:\n",
    "#     # Process the raw data into a DataFrame, drop nans and add indicators\n",
    "#     df = collector.process_data_to_dataframe(raw_data)\n",
    "#     df = collector.fix_data()\n",
    "#     df = collector.add_all_indicators_finta()\n",
    "    \n",
    "#     # Save the DataFrame to a CSV file\n",
    "#     filename = f'{symbol}_{interval}_{days_back}days.csv'\n",
    "#     collector.save_data(df, filename)\n",
    "\n",
    "#     print(f\"Данные успешно собраны и сохранены в файл {filename}\")\n",
    "# else:\n",
    "#     print(\"Не удалось собрать данные\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60a7345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph data from a CSV file\n",
    "dirname = 'data'\n",
    "filename = os.path.join(dirname, f'BTCUSDT_{interval}_{days_back}days.csv')\n",
    "\n",
    "graph_data = pd.read_csv(filename)\n",
    "graph_data = graph_data.rename(columns={'open_time':'date_time'}) # Rename 'open_time' to 'date_time' for consistency\n",
    "graph_data['date_time'] = pd.to_datetime(graph_data['date_time']) # Convert 'date_time' to datetime format\n",
    "\n",
    "graph_data.sort_values('date_time', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f0f1572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105014/105014 [00:19<00:00, 5328.71it/s] \n"
     ]
    }
   ],
   "source": [
    "# Merge graph data with news data based on date_time, using binary search for efficiency\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "# Constants for merging\n",
    "RESULT_FILENAME = 'result_data.csv'\n",
    "DIRNAME = 'result_train_data'\n",
    "THRESHOLD = pd.Timedelta('1d').value \n",
    "SEP = '\\n' # Separator for news items per graph point\n",
    "\n",
    "# Create directory for results if it doesn't exist\n",
    "os.makedirs(DIRNAME, exist_ok=True)\n",
    "PATH_TO_FILE = os.path.join(DIRNAME, RESULT_FILENAME)\n",
    "\n",
    "# Prepare news data for merging\n",
    "filtered_news = filtered_news.reset_index(drop=True)\n",
    "news_texts = filtered_news.apply(lambda row: row['title'] if pd.isna(row['article_text']) else row['title'] + ' ' + row['article_text'], axis=1).astype(str).values # Combine title and article_text into a single string for each news item <------- may experiment with title and article_text separator (or just use title or article_text)\n",
    "news_texts = filtered_news.apply(lambda row: row['title'], axis=1).astype(str).values # Use only title for news items for space efficiency <------- delete this line if you want to use both title and article_text\n",
    "\n",
    "# Extract date_time values from graph data and news data\n",
    "graph_date = graph_data['date_time'].reset_index(drop=True).values\n",
    "news_date = filtered_news['date_time'].values\n",
    "\n",
    "\n",
    "# Perform binary search to find the starting indices of news articles for each graph date as anchors\n",
    "start_indices = np.searchsorted(graph_date, news_date, side='left')\n",
    "\n",
    "prev_left = 0\n",
    "columns = list(graph_data.columns) + ['news']\n",
    "\n",
    "# Writing merged data to CSV file\n",
    "with open(PATH_TO_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(columns)\n",
    "\n",
    "    # Iterate through each date in the graph data and find corresponding news using binary search (searchsorted)\n",
    "    for i in tqdm(range(len(graph_date))):\n",
    "        current_date = graph_date[i]\n",
    "        right_idx = start_indices[i]\n",
    "        \n",
    "        min_date = current_date - THRESHOLD\n",
    "        left_idx = np.searchsorted(news_date[prev_left:right_idx], min_date, side='left') + prev_left\n",
    "        news_to_append = 'NONE'\n",
    "\n",
    "        # Check if there are news articles within the threshold\n",
    "        if left_idx < right_idx:\n",
    "            news_slice = news_texts[left_idx:right_idx]\n",
    "            news_to_append = SEP.join(news_slice) # <------- may experiment with different separators for news items\n",
    "        \n",
    "        row_to_append = list(graph_data.iloc[i].values) + [news_to_append]\n",
    "        writer.writerow(row_to_append)\n",
    "        prev_left = left_idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stockpricevenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
