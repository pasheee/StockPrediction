{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5420c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from graphsParsing.graphs_parsing_tools import BinanceDataCollector\n",
    "import os\n",
    "\n",
    "\n",
    "# Define parameters for graph data collection\n",
    "symbol = 'BTCUSDT'\n",
    "interval = '5m'  \n",
    "days_back = 730 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f815fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "news = pd.read_csv('./train_news_dataset/merged_news.csv')\n",
    "\n",
    "# Convert 'date_time' to datetime format and filter out rows with invalid date formats\n",
    "news['date_length'] = news['date_time'].apply(lambda x: len(x)) \n",
    "filtered_news = news[news['date_length'] == 19] # filtering for date_time strings of length 19 (to convert to datetime, because 3 of them are not the same format)\n",
    "filtered_news = filtered_news.drop(columns=['date_length'])\n",
    "filtered_news['date_time'] = pd.to_datetime(filtered_news['date_time'])\n",
    "\n",
    "filtered_news.to_csv('newsAndPriceMapping/news.csv', index=False)\n",
    "\n",
    "# Sort the DataFrame by 'date_time'\n",
    "filtered_news = filtered_news.sort_values('date_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628f017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем сбор данных...\n",
      "Данные успешно собраны и сохранены в файл BTCUSDT_5m_365days.csv\n"
     ]
    }
   ],
   "source": [
    "# Collect historical data using BinanceDataCollector <------- UNCOMMENT THIS PART TO RUN DATA COLLECTION \n",
    "# collector = BinanceDataCollector()\n",
    "\n",
    "# symbol = 'BTCUSDT'  \n",
    "# interval = '5m'     \n",
    "# days_back = 365 \n",
    "\n",
    "# print(\"Начинаем сбор данных...\")\n",
    "# raw_data = collector.collect_historical_data(symbol, interval, days_back)\n",
    "\n",
    "# if raw_data:\n",
    "#     # Process the raw data into a DataFrame, drop nans and add indicators\n",
    "#     df = collector.process_data_to_dataframe(raw_data)\n",
    "#     df = collector.fix_data()\n",
    "#     df = collector.add_all_indicators_finta()\n",
    "    \n",
    "#     # Save the DataFrame to a CSV file\n",
    "#     filename = f'{symbol}_{interval}_{days_back}days.csv'\n",
    "#     collector.save_data(df, filename)\n",
    "\n",
    "#     print(f\"Данные успешно собраны и сохранены в файл {filename}\")\n",
    "# else:\n",
    "#     print(\"Не удалось собрать данные\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d60a7345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph data from a CSV file\n",
    "dirname = 'data'\n",
    "filename = os.path.join(dirname, f'BTCUSDT_{interval}_{days_back}days.csv')\n",
    "\n",
    "graph_data = pd.read_csv(filename)\n",
    "graph_data = graph_data.rename(columns={'open_time':'date_time'}) # Rename 'open_time' to 'date_time' for consistency\n",
    "graph_data['date_time'] = pd.to_datetime(graph_data['date_time']) # Convert 'date_time' to datetime format\n",
    "\n",
    "graph_data.sort_values('date_time', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f1572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 106321/210030 [00:09<00:09, 10702.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     56\u001b[39m row_to_append = \u001b[38;5;28mlist\u001b[39m(graph_data.iloc[i].values) + [news_to_append]\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_to_append\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m prev_left = left_idx\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Merge graph data with news data based on date_time, using binary search for efficiency\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "# Constants for merging\n",
    "RESULT_FILENAME = 'result_data.csv'\n",
    "DIRNAME = 'result_train_data'\n",
    "THRESHOLD = pd.Timedelta('1d').value \n",
    "SEP = '\\n' # Separator for news items per graph point\n",
    "\n",
    "# Create directory for results if it doesn't exist\n",
    "os.makedirs(DIRNAME, exist_ok=True)\n",
    "PATH_TO_FILE = os.path.join(DIRNAME, RESULT_FILENAME)\n",
    "\n",
    "# Prepare news data for merging\n",
    "filtered_news = filtered_news.reset_index(drop=True)\n",
    "news_texts = filtered_news.apply(lambda row: row['title'] if pd.isna(row['article_text']) else row['title'] + ' ' + row['article_text'], axis=1).astype(str).values # Combine title and article_text into a single string for each news item <------- may experiment with title and article_text separator (or just use title or article_text)\n",
    "news_texts = filtered_news.apply(lambda row: row['title'], axis=1).astype(str).values # Use only title for news items for space efficiency <------- delete this line if you want to use both title and article_text\n",
    "\n",
    "# Extract date_time values from graph data and news data\n",
    "graph_date = graph_data['date_time'].reset_index(drop=True).values\n",
    "news_date = filtered_news['date_time'].values\n",
    "\n",
    "\n",
    "# Perform binary search to find the starting indices of news articles for each graph date as anchors\n",
    "start_indices = np.searchsorted(graph_date, news_date, side='left')\n",
    "\n",
    "prev_left = 0\n",
    "columns = list(graph_data.columns) + ['news']\n",
    "\n",
    "# Writing merged data to CSV file\n",
    "with open(PATH_TO_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(columns)\n",
    "\n",
    "    # Iterate through each date in the graph data and find corresponding news using binary search (searchsorted)\n",
    "    for i in tqdm(range(len(graph_date))):\n",
    "        current_date = graph_date[i]\n",
    "        right_idx = start_indices[i]\n",
    "        \n",
    "        min_date = current_date - THRESHOLD\n",
    "        left_idx = np.searchsorted(news_date[prev_left:right_idx], min_date, side='left') + prev_left\n",
    "        news_to_append = 'NONE'\n",
    "\n",
    "        # Check if there are news articles within the threshold\n",
    "        if left_idx < right_idx:\n",
    "            news_slice = news_texts[left_idx:right_idx]\n",
    "            news_to_append = SEP.join(news_slice) # <------- may experiment with different separators for news items\n",
    "        \n",
    "        row_to_append = list(graph_data.iloc[i].values) + [news_to_append]\n",
    "        writer.writerow(row_to_append)\n",
    "        prev_left = left_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb8487",
   "metadata": {},
   "source": [
    "# With vectorized articles text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce59226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "from typing import Optional, List, Union\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94596ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cdbb2049b84d0d954296ae4743024a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shari\\PycharmProjects\\StockPrediction\\stockpricevenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shari\\.cache\\huggingface\\hub\\models--ProsusAI--finbert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c279a62ba540bf92fa7ce2b253da3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25258f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_FILENAME = 'result_data_vectorized_news.csv'\n",
    "DIRNAME = 'result_train_data'\n",
    "THRESHOLD = pd.Timedelta('1d').value\n",
    "MAX_LENGTH = 1024\n",
    "SEP = '[SEP]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6863c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2011-06-22T10:56:00.000000000', '2011-06-22T10:56:00.000000000',\n",
       "       '2011-06-22T10:56:00.000000000', ...,\n",
       "       '2025-06-03T22:31:49.000000000', '2025-06-03T22:31:49.000000000',\n",
       "       '2025-06-04T14:01:05.000000000'],\n",
       "      shape=(239161,), dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      0,      0, ..., 199453, 199453, 199639],\n",
       "      shape=(239161,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8337adba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 29483/210030 [01:26<08:51, 339.56it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m     embedding = np.zeros(\u001b[32m768\u001b[39m)\n\u001b[32m     55\u001b[39m row_to_append = \u001b[38;5;28mlist\u001b[39m(graph_data.iloc[i].values) + [embedding]\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_to_append\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m prev_left = left_idx\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create directory for results if it doesn't exist\n",
    "os.makedirs(DIRNAME, exist_ok=True)\n",
    "PATH_TO_FILE = os.path.join(DIRNAME, RESULT_FILENAME)\n",
    "\n",
    "# Prepare news data for merging\n",
    "filtered_news = filtered_news.reset_index(drop=True)\n",
    "news_texts = filtered_news.apply(lambda row: row['title'] if pd.isna(row['article_text']) else row['title'] + ': ' + row['article_text'], axis=1).astype(str).values # Combine title and article_text into a single string for each news item <------- may experiment with title and article_text separator (or just use title or article_text)\n",
    "\n",
    "# Extract date_time values from graph data and news data\n",
    "graph_date = graph_data['date_time'].reset_index(drop=True).values\n",
    "news_date = filtered_news['date_time'].values\n",
    "\n",
    "\n",
    "# Perform binary search to find the starting indices of news articles for each graph date as anchors\n",
    "start_indices = np.searchsorted(graph_date, news_date, side='left')\n",
    "\n",
    "prev_left = 0\n",
    "columns = list(graph_data.columns) + ['news']\n",
    "\n",
    "# Writing merged data to CSV file\n",
    "with open(PATH_TO_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(columns)\n",
    "\n",
    "    # Iterate through each date in the graph data and find corresponding news using binary search (searchsorted)\n",
    "    for i in tqdm(range(len(graph_date))):\n",
    "        current_date = graph_date[i]\n",
    "        right_idx = start_indices[i]\n",
    "        \n",
    "        min_date = current_date - THRESHOLD\n",
    "        left_idx = np.searchsorted(news_date[prev_left:right_idx], min_date, side='left') + prev_left\n",
    "        news_to_append = 'NONE'\n",
    "\n",
    "        # Check if there are news articles within the threshold\n",
    "        if left_idx < right_idx:\n",
    "            news_slice = news_texts[left_idx:right_idx]\n",
    "            news_to_append = SEP.join(news_slice) # <------- may experiment with different separators for news \n",
    "            \n",
    "            tokenized_data = tokenizer(\n",
    "                news_to_append,\n",
    "                return_tensors='np',\n",
    "                max_length=MAX_LENGTH,\n",
    "                truncation=True,\n",
    "            )\n",
    "\n",
    "            outputs = model(**tokenized_data)\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten()\n",
    "\n",
    "            print(embedding)\n",
    "            break\n",
    "        else:\n",
    "            embedding = np.zeros(768)\n",
    "\n",
    "        row_to_append = list(graph_data.iloc[i].values) + [embedding]\n",
    "        writer.writerow(row_to_append)\n",
    "        prev_left = left_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd36b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stockpricevenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
